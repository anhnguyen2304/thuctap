{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b9536f78",
      "metadata": {
        "id": "b9536f78"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "# Tải các mô hình nén từ tensorflow_hub\n",
        "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
        "\n",
        "import IPython.display as display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "\n",
        "import PIL.Image\n",
        "import time\n",
        "import functools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class neuran_style_tranfer:\n",
        "  def __init__ (self,content_path,style_path):\n",
        "    self.content_image = self.load_img(content_path)\n",
        "    self.style_image = self.load_img(style_path)\n",
        "\n",
        "    content_layers = ['block5_conv2'] \n",
        "\n",
        "    style_layers = ['block1_conv1',\n",
        "                    'block2_conv1',\n",
        "                    'block3_conv1', \n",
        "                    'block4_conv1', \n",
        "                    'block5_conv1']\n",
        "\n",
        "    self.num_content_layers = len(content_layers)\n",
        "    self.num_style_layers = len(style_layers)\n",
        "\n",
        "    self.style_extractor = self.vgg_layers(style_layers)\n",
        "    self.style_outputs = self.style_extractor(self.style_image*255)   \n",
        "\n",
        "    self.extractor = self.StyleContentModel(style_layers, content_layers)\n",
        "\n",
        "    self.results = self.extractor(tf.constant(self.content_image)) \n",
        "\n",
        "    self.style_targets = self.extractor(self.style_image)['style'] \n",
        "    self.content_targets = self.extractor(self.content_image)['content']\n",
        "\n",
        "    self.image = tf.Variable(self.content_image)\n",
        "\n",
        "    self.opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
        "    self.style_weight=1e-2\n",
        "    self.content_weight=1e4  \n",
        "\n",
        "  def tensor_to_image(self,tensor):\n",
        "    tensor = tensor*255\n",
        "    tensor = np.array(tensor, dtype=np.uint8)\n",
        "    if np.ndim(tensor)>3:\n",
        "      assert tensor.shape[0] == 1\n",
        "      tensor = tensor[0]\n",
        "    return PIL.Image.fromarray(tensor)\n",
        "\n",
        "  def load_img(self, path_to_img):\n",
        "    max_dim = 512\n",
        "    img = tf.io.read_file(path_to_img)\n",
        "    img = tf.image.decode_image(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "    long_dim = max(shape)\n",
        "    scale = max_dim / long_dim\n",
        "\n",
        "    new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "    img = tf.image.resize(img, new_shape)\n",
        "    self.img = img[tf.newaxis, :]\n",
        "    return self.img\n",
        "\n",
        "  def vgg_layers(self, layer_names):\n",
        "    \"\"\" Creates a VGG model that returns a list of intermediate output values.\"\"\"\n",
        "    # Load our model. Load pretrained VGG, trained on ImageNet data\n",
        "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "    vgg.trainable = False\n",
        "    \n",
        "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "    self.model = tf.keras.Model([vgg.input], outputs)\n",
        "    return self.model\n",
        "\n",
        "  def gram_matrix(sefl,input_tensor):\n",
        "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "    input_shape = tf.shape(input_tensor)\n",
        "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "    return result/(num_locations)\n",
        "\n",
        "  def style_content_loss(self,outputs):\n",
        "    style_outputs = outputs['style']\n",
        "    content_outputs = outputs['content']\n",
        "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-self.style_targets[name])**2) \n",
        "                           for name in style_outputs.keys()])\n",
        "    style_loss *= self.style_weight / self.num_style_layers\n",
        "\n",
        "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-self.content_targets[name])**2) \n",
        "                             for name in content_outputs.keys()])\n",
        "    content_loss *= self.content_weight / self.num_content_layers\n",
        "    self.loss = style_loss + content_loss\n",
        "    return self.loss\n",
        "\n",
        "  class StyleContentModel(tf.keras.models.Model):\n",
        "    def __init__(self, style_layers, content_layers):\n",
        "      self.vgg = neuran_style_tranfer.vgg_layers(layer_names= (style_layers + content_layers))\n",
        "      self.style_layers = style_layers\n",
        "      self.content_layers = content_layers\n",
        "      self.num_style_layers = len(style_layers)\n",
        "      self.vgg.trainable = False\n",
        "\n",
        "    def call(self, inputs):\n",
        "      \"Expects float input in [0,1]\"\n",
        "      inputs = inputs*255.0\n",
        "      preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
        "      outputs = self.vgg(preprocessed_input)\n",
        "      style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
        "                                        outputs[self.num_style_layers:])\n",
        "\n",
        "      style_outputs = [self.gram_matrix(style_output)\n",
        "                      for style_output in style_outputs]\n",
        "\n",
        "      self.content_dict = {content_name: value\n",
        "                      for content_name, value\n",
        "                      in zip(self.content_layers, content_outputs)}\n",
        "\n",
        "      self.style_dict = {style_name: value\n",
        "                    for style_name, value\n",
        "                    in zip(self.style_layers, style_outputs)}\n",
        "\n",
        "      return {'content': self.content_dict, 'style': self.style_dict}\n",
        "\n",
        "  def clip_0_1(self,image):\n",
        "    return tf.clip_by_value(self.image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "  @tf.function()\n",
        "  def train_step(self,image):\n",
        "    # tính đạo hàm\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Tìm giá trị dự đoán và tính giá trị tổn thất\n",
        "      outputs = self.extractor(image)\n",
        "      loss = self.style_content_loss(outputs)\n",
        "  #Tính toán độ dốc\n",
        "    grad = tape.gradient(loss, image) \n",
        "    #Yêu cầu trình tối ưu hóa áp dụng các chuyển màu đã xử lý.\n",
        "    self.opt.apply_gradients([(grad, image)])\n",
        "    # grad: độ dốc; image: biến\n",
        "    # Trả lại:Một Operation áp dụng các gradient được chỉ định. Các lần lặp sẽ được tự động tăng thêm 1.\n",
        "    self.image.assign(self.clip_0_1(image)) # cập nhật\n",
        "\n",
        "  def sanpham(self):\n",
        "    epochs = 10\n",
        "    steps_per_epoch = 100\n",
        "\n",
        "    for n in range(epochs):\n",
        "      for m in range(steps_per_epoch):\n",
        "        self.train_step(self.image)\n",
        "      display.clear_output(wait=True)\n",
        "      display.display(self.tensor_to_image(self.image))\n",
        "\n",
        "  def return_img(self):\n",
        "    return self.tensor_to_image(self.image)"
      ],
      "metadata": {
        "id": "8erE7LBJlDqM"
      },
      "id": "8erE7LBJlDqM",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(image, title=None):\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)"
      ],
      "metadata": {
        "id": "4hGJXVjjvFak"
      },
      "id": "4hGJXVjjvFak",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_path = tf.keras.utils.get_file('d3305f1a76111419459a883a11202fac.jpg','https://i.ibb.co/dJ6gYSv/d3305f1a76111419459a883a11202fac.jpg')\n",
        "style_path = tf.keras.utils.get_file('test.jpg','https://i.ibb.co/PmdRzKZ/test.jpg')"
      ],
      "metadata": {
        "id": "TfRXiX6Xtipl"
      },
      "id": "TfRXiX6Xtipl",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = neuran_style_tranfer(content_path,style_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "d0xjai-pK9yE",
        "outputId": "431cdfc4-98ec-441b-b521-68f0e7a47d7a"
      },
      "id": "d0xjai-pK9yE",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-1ed4306eb2b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuran_style_tranfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstyle_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-0e054a8e971a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, content_path, style_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_image\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStyleContentModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-0e054a8e971a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, style_layers, content_layers)\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0mStyleContentModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuran_style_tranfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstyle_layers\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcontent_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstyle_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: vgg_layers() missing 1 required positional argument: 'self'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UZndrA8hOcYV"
      },
      "id": "UZndrA8hOcYV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}